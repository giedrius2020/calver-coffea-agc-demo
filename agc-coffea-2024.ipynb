{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688000d2-0ab3-4ff8-a4cc-f112422bac42",
   "metadata": {},
   "source": [
    "# AGC + calver coffea on coffea-casa\n",
    "\n",
    "We'll base this on a few sources:\n",
    "- https://github.com/iris-hep/analysis-grand-challenge/tree/main/analyses/cms-open-data-ttbar (AGC, of course)\n",
    "- https://github.com/alexander-held/CompHEP-2023-AGC (contains a simplified version of AGC)\n",
    "- https://github.com/nsmith-/TTGamma_LongExercise/ (credit Nick Smith for helpful examples of the new API)\n",
    "- (and if time allows, weight features: https://github.com/CoffeaTeam/coffea/blob/backports-v0.7.x/binder/accumulators.ipynb / https://coffeateam.github.io/coffea/api/coffea.analysis_tools.Weights.html#coffea.analysis_tools.Weights.partial_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:38.782002Z",
     "start_time": "2024-08-01T15:10:37.422997Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward: 2.6.7\n",
      "dask-awkward: 2024.7.0\n",
      "dask: 2024.8.1\n",
      "uproot: 5.3.11.dev75+g4e5ea77\n",
      "hist: 2.7.3\n",
      "coffea: 2024.8.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "import traceback\n",
    "from dask.distributed import Client\n",
    "import skhep_testdata\n",
    "import pandas as pd\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "utils.plotting.set_style()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "\n",
    "# client = Client(\"tls://localhost:8786\")\n",
    "\n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"dask-awkward: {dak.__version__}\")\n",
    "print(f\"dask: {dask.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b8cc4-ee4a-4294-86d8-f5db8cf15aaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Produce an AGC histogram with Dask (no coffea yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e667cbdf-9e6f-4c7b-8827-2f6bb35c670b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:38.792459Z",
     "start_time": "2024-08-01T15:10:38.783403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_trijet_mass(events):\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electron[events.Electron.pt > 30 & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    print(\"Selected electrons: \", selected_electrons)\n",
    "    selected_muons = events.Muon[events.Muon.pt > 30 & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[events.Jet.pt > 25 & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return ak.flatten(trijet_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c972c",
   "metadata": {},
   "source": [
    "Reading in the ROOT file, we can now create a Dask task graph for the calculations and plot that we want to make using `dask-awkward` and `hist.dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a00374a-83d5-48b8-8e68-7097936170ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.083914Z",
     "start_time": "2024-08-01T15:10:38.838844Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was loaded with uproot, event count:  947\n",
      "File was loaded with uproot, event count:  947\n"
     ]
    }
   ],
   "source": [
    "all_files = {}\n",
    "events_list = []\n",
    "\n",
    "# Some files are downloaded locally:\n",
    "# all_files.append(ttbar_file)\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\") # ttbar remote 533M size\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\") # RNTuple remote\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\") # TTree remote\n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/rntuple_v1.root\") # RNTuple, local, with our own converter v4 \n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/rntuple_v2.root\") # RNTuple, local, with our own converter v4 \n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/rntuple_v3.root\") # RNTuple, local, with our own converter v4 \n",
    "\n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/rntuple_v4.root\") # RNTuple, local, with our own converter v4 \n",
    "\n",
    "\n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\") # TTree ttbar original\n",
    "all_files[\"TT\"] = \"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\" # TTree local\n",
    "# all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\"  # RNTuple local\n",
    "# all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple_v1.root\" # RNTuple, local, with our own converter v4 \n",
    "# all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple_v2.root\" # RNTuple, local, with our own converter v4 \n",
    "# all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple_v3.root\" # RNTuple, local, with our own converter v4 \n",
    "# all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple_v4.root\" # RNTuple, local, with our own converter v4 \n",
    "# all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple_0903_v5.root\" # RNTuple, local, with our own converter v5\n",
    "all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple_v6_632_0909.root\" # RNTuple, local, with our own converter v5\n",
    "# all_files[\"RN\"] = \"/home/cms-jovyan/my_root_files/rntuple_v7_6_0909.root\" # RNTuple, local, with our own converter v5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_files_with_uproot(files):\n",
    "    for fl in files.values():\n",
    "        with uproot.open(fl) as f:\n",
    "            events = f[\"Events\"]\n",
    "            events_list.append(events)\n",
    "            print(\"File was loaded with uproot, event count: \", len(events.keys()))\n",
    "            \n",
    "            # NOTE: to access array: # events.arrays([\"Electron_pt\"])[\"Electron_pt\"]\n",
    "        \n",
    "def load_files_with_coffea(files):\n",
    "    for fl in files:\n",
    "        events = NanoEventsFactory.from_root({fl: \"Events\"}, schemaclass=NanoAODSchema).events()\n",
    "        events_list.append(events)\n",
    "        print(\"File was loaded with coffea, fields count: \", len(events.fields))\n",
    "        \n",
    "load_files_with_uproot(all_files)\n",
    "\n",
    "# load_files_with_coffea(all_files)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1800f9-82ab-44c2-9344-62a7ff6e617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Various available properties:\n",
    "# print(\"Name: \", events.name)\n",
    "# print(\"header: \", events.header)\n",
    "# print(\"footer: \", events.footer)\n",
    "# print(\"num_entries: \", events.num_entries)\n",
    "# print(\"len of field_names: \", len(events.field_names))\n",
    "# print(\"keys: \", len(events.keys()))\n",
    "# print(\" field_names: \", events.fields)\n",
    "# print(\"column_records: \", events.column_records[:10])\n",
    "# print(\"keys: \", events.keys()[:10])\n",
    "# print(\"_column_records_dict: \", events._column_records_dict)\n",
    "# print(\"_related_ids: \", events._related_ids)\n",
    "# print(\"page_list_envelopes: \", events.page_list_envelopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ee5919-2dcb-49f9-8065-2967a192620a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to timeit on various functions: \n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "events_dict = {}\n",
    "\n",
    "def format_test_results(times):\n",
    "    df = pd.DataFrame(times, columns =['data_type', 'func_name', 'time(s)'])\n",
    "    df = df.sort_values(by=['func_name'])\n",
    "    df['time(s)'] = df['time(s)'].round(4)\n",
    "    df['func_name'] = df['func_name'].str.replace('_', ' ', regex=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Starting to timeit on various functions: \")\n",
    "\n",
    "def load_file(data_type, file):\n",
    "    with uproot.open(file) as f:\n",
    "        events = f[\"Events\"]\n",
    "        events_dict[data_type] = events\n",
    "\n",
    "def load_arrays_for_each_key(events):\n",
    "    for key in events.keys():\n",
    "        events.arrays(filter_name=[key])[key]\n",
    "        \n",
    "def load_all_arrays(events):\n",
    "    events.arrays()\n",
    "    \n",
    "def load_all_arrays_while_using_filter_name(events):\n",
    "    chosen_keys = events.keys()\n",
    "    events.arrays(filter_name=chosen_keys)[chosen_keys]\n",
    "\n",
    "def load_array_while_using_filter_name(events):\n",
    "    key = \"nGenVisTau\"\n",
    "    events.arrays(filter_name=[key])[key]\n",
    "    \n",
    "    \n",
    "def load_10_arrays_while_using_filter_name(events):\n",
    "    chosen_keys = [ \"nGenVisTau\",\n",
    "                    \"GenVisTau_eta\",\n",
    "                    \"GenVisTau_mass\",\n",
    "                    \"GenVisTau_phi\",\n",
    "                    \"GenVisTau_pt\",\n",
    "                    \"GenVisTau_charge\",\n",
    "                    \"GenVisTau_genPartIdxMother\",\n",
    "                    \"GenVisTau_status\",\n",
    "                    \"genWeight\",\n",
    "                    \"LHEWeight_originalXWGTUP\"]\n",
    "    \n",
    "    events.arrays(filter_name=chosen_keys)[chosen_keys]\n",
    "        \n",
    "def start_all_performance_tests():\n",
    "    times = []\n",
    "    \n",
    "    for data_type, file in all_files.items():\n",
    "        time_taken = timeit.timeit(lambda: load_file(data_type, file), number=1)\n",
    "        times.append((data_type, \"load_file\", time_taken))\n",
    "\n",
    "        time_taken = timeit.timeit(lambda: load_arrays_for_each_key(events_dict[data_type]), number=1)\n",
    "        times.append((data_type, \"load_arrays_for_each_key\", time_taken))\n",
    "        \n",
    "        time_taken = timeit.timeit(lambda: load_all_arrays(events_dict[data_type]), number=1)\n",
    "        times.append((data_type, \"load_all_arrays\", time_taken))\n",
    "        \n",
    "        time_taken = timeit.timeit(lambda: load_all_arrays_while_using_filter_name(events_dict[data_type]), number=1)\n",
    "        times.append((data_type, \"load_all_arrays_while_using_filter_name\", time_taken))\n",
    "        \n",
    "        time_taken = timeit.timeit(lambda: load_10_arrays_while_using_filter_name(events_dict[data_type]), number=1)\n",
    "        times.append((data_type, \"load_10_arrays_while_using_filter_name\", time_taken))\n",
    "        \n",
    "        time_taken = timeit.timeit(lambda: load_array_while_using_filter_name(events_dict[data_type]), number=1)\n",
    "        times.append((data_type, \"load_array_while_using_filter_name\", time_taken))\n",
    "\n",
    "    \n",
    "    return format_test_results(times)\n",
    "\n",
    "results = start_all_performance_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c29735-610a-4bef-95e2-80fba57ea1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type                               func_name  time(s)\n",
      "       TT  load 10 arrays while using filter name   0.1036\n",
      "       RN  load 10 arrays while using filter name   0.0926\n",
      "       TT                         load all arrays  49.4377\n",
      "       RN                         load all arrays   4.1201\n",
      "       TT load all arrays while using filter name  53.6159\n",
      "       RN load all arrays while using filter name   3.9096\n",
      "       TT      load array while using filter name   0.0146\n",
      "       RN      load array while using filter name   0.0570\n",
      "       TT                load arrays for each key  20.6384\n",
      "       RN                load arrays for each key  53.9863\n",
      "       TT                               load file   0.3144\n",
      "       RN                               load file   0.0009\n"
     ]
    }
   ],
   "source": [
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58212611-abcb-42ed-96fc-9ad0a0e61d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTree keys length: 947. RNTuple keys length: 947\n"
     ]
    }
   ],
   "source": [
    "# This cell compares keys for TTree and RNTuple. For each matching key, it compares all array values. At the end, comparison statistics are printed.\n",
    "def compare_key_lists(ls1, ls2):\n",
    "    match_count = 0\n",
    "    mismatch_count = 0\n",
    "    \n",
    "    ak_match_count = 0\n",
    "    ak_mismatch_count = 0\n",
    "    ak_error_count = 0\n",
    "    \n",
    "    count_of_all_tt_elements = 0\n",
    "    count_of_all_rn_elements = 0\n",
    "    \n",
    "    for i in range(len(ls1)):\n",
    "        if keys_tt[i] == keys_rn[i]:\n",
    "            key = keys_tt[i]\n",
    "            match_count+=1\n",
    "\n",
    "            arrays_tt = events_tt.arrays([key])[key]\n",
    "            arrays_rn = events_rn.arrays([key])[key]\n",
    "            \n",
    "            el_count_tt = len(ak.ravel(arrays_tt))\n",
    "            el_count_rn = len(ak.ravel(arrays_rn))\n",
    "            \n",
    "\n",
    "            # Check if arrays are equal:\n",
    "            try:                \n",
    "                # Custom function to compare NaN-aware equality\n",
    "                def nan_equal(x, y):\n",
    "                    if isinstance(x, (list, ak.Array)) and isinstance(y, (list, ak.Array)):\n",
    "                        return all(nan_equal(a, b) for a, b in zip(x, y))\n",
    "                    return (x == y) or (np.isnan(x) and np.isnan(y))\n",
    "                # Check if the lengths of the outermost arrays are equal\n",
    "                assert len(arrays_tt) == len(arrays_rn)\n",
    "                \n",
    "                # Compare the arrays using the custom function\n",
    "                are_equal = nan_equal(arrays_tt.tolist(), arrays_rn.tolist())\n",
    "                \n",
    "                if are_equal:\n",
    "                    ak_match_count += 1\n",
    "                    print(f\"[{key}]\", \"ak arrays are equal\")\n",
    "                elif not are_equal:\n",
    "                    count_of_all_tt_elements+=el_count_tt\n",
    "                    count_of_all_rn_elements+=el_count_rn\n",
    "                    ak_mismatch_count += 1\n",
    "                    print(f\"[{key}]\", \"ak comparison MISMATCH\")\n",
    "                    print(\"tt: \", arrays_tt, f\"Type: {ak.type(arrays_tt)}. Count of elements: {el_count_tt}\")\n",
    "                    print(\"rn: \", arrays_rn, f\"Type: {ak.type(arrays_rn)}. Count of elements: {el_count_rn}\")\n",
    "                \n",
    "            except:\n",
    "                count_of_all_tt_elements+=el_count_tt\n",
    "                count_of_all_rn_elements+=el_count_rn\n",
    "                ak_error_count += 1\n",
    "                print(f\"[{key}]\", \"ak comparison ERROR\")\n",
    "                print(\"tt: \", arrays_tt, f\"Type: {ak.type(arrays_tt)}. Count of elements: {el_count_tt}\")\n",
    "                print(\"rn: \", arrays_rn, f\"Type: {ak.type(arrays_rn)}. Count of elements: {el_count_rn}\")\n",
    "        else:\n",
    "            mismatch_count+=1\n",
    "            # print(\"Mismatch: \", keys_tt[i], \"---\", keys_rn[i])\n",
    "    print(f\"Keys comparison statistics: matched count: {match_count}; mismatch count: {mismatch_count}\")\n",
    "    print(f\"ak array comparison statistics: matched count: {ak_match_count}; mismatch count: {ak_mismatch_count}; errors: {ak_error_count}\")\n",
    "\n",
    "\n",
    "    \n",
    "events_tt = events_list[0]\n",
    "events_rn = events_list[1]\n",
    "\n",
    "# # Must be sorted, because otherwise the order is different.\n",
    "keys_tt = sorted(events_tt.keys(), key=str.lower)\n",
    "keys_rn = sorted(events_rn._keys, key=str.lower)\n",
    "\n",
    "print(f\"TTree keys length: {len(keys_tt)}. RNTuple keys length: {len(keys_rn)}\")\n",
    "# compare_key_lists(keys_tt, keys_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c19e25-72c1-4d62-b15f-6ba332049573",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "def collect_breaking_points(key):\n",
    "    cluster_starts = [md.num_first_entry for md in events_rn.cluster_summaries][1:] # Skip first, because it is 0.\n",
    "    print(\"Starts of clusters: \", cluster_starts)\n",
    "\n",
    "    step = 4\n",
    "    for cl_start in cluster_starts:\n",
    "        for i in range (cl_start-19, cl_start+19, step):\n",
    "            strt = i\n",
    "            end = i + step\n",
    "            arr_tt = events_tt.arrays(filter_name=[key], entry_start=strt, entry_stop=end)[key]\n",
    "            arr_rn = events_rn.arrays(filter_name=[key], entry_start=strt, entry_stop=end)[key]\n",
    "            \n",
    "            try:\n",
    "                 # Custom function to compare NaN-aware equality\n",
    "                def nan_equal(x, y):\n",
    "                    if isinstance(x, (list, ak.Array)) and isinstance(y, (list, ak.Array)):\n",
    "                        return all(nan_equal(a, b) for a, b in zip(x, y))\n",
    "                    return (x == y) or (np.isnan(x) and np.isnan(y))\n",
    "                # Check if the lengths of the outermost arrays are equal\n",
    "                assert len(arr_tt) == len(arr_rn)\n",
    "                \n",
    "                # Compare the arrays using the custom function\n",
    "                are_equal = nan_equal(arr_tt.tolist(), arr_rn.tolist())\n",
    "                assert(are_equal)\n",
    "                # print(\"EQUAL:\")\n",
    "                # print(f\"TT array: {ak.to_list(arr_tt)}\")\n",
    "                # print(f\"RN array: {ak.to_list(arr_rn)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"TT array: {arr_tt}\")\n",
    "                print(f\"RN array: {arr_rn}\")\n",
    "                print(\"Index: \", i, f\". Failure limits: {(strt, end)}\")\n",
    "                print(\"\")\n",
    "\n",
    "key = \"Electron_hoe\"\n",
    "# collect_breaking_points(key)\n",
    "print(uproot.const.rntuple_col_type_to_num_dict[\"splitindex32\"])\n",
    "# print(\"Finished cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80cec75-8c1f-453d-a466-a9c6d546b8b2",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(events_tt.keys(filter_name=[\"run\", \"Electron_hoe\"]))\n",
    "# print(events_rn.keys(filter_name=[\"run\", \"Electron_hoe\"]))\n",
    "# print(events_rn.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cb5c9e-e501-4e7b-8893-ad7fc0dc31df",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts of clusters:  [17224, 44433, 71709, 98997, 126243, 153500, 180739]\n",
      "TT: [[0], [0.0213, 0.0372], [0, 0], [0, ...], ..., [], [0, 0.0581], [0, 1.08, 12.5]]\n",
      "RN: [[0], [0.0213, 0.0372], [0, 0], [0, ...], ..., [], [0, 0.0581], [0, 1.08, 12.5]]\n"
     ]
    }
   ],
   "source": [
    "def compare_array_region(key, events_tt, events_rn):\n",
    "    cluster_starts = [md.num_first_entry for md in events_rn.cluster_summaries][1:] # Skip first, because it is 0.\n",
    "    print(\"Starts of clusters: \", cluster_starts)\n",
    "    \n",
    "    strt = 44431\n",
    "    end = 44450\n",
    "\n",
    "    arr_tt = events_tt.arrays(filter_name=[key], entry_start=strt, entry_stop=end)[key]\n",
    "    arr_rn = events_rn.arrays(filter_name=[key], entry_start=strt, entry_stop=end)[key]\n",
    "\n",
    "    try:\n",
    "        # Custom function to compare NaN-aware equality\n",
    "        def nan_equal(x, y):\n",
    "            if isinstance(x, (list, ak.Array)) and isinstance(y, (list, ak.Array)):\n",
    "                return all(nan_equal(a, b) for a, b in zip(x, y))\n",
    "            return (x == y) or (np.isnan(x) and np.isnan(y))\n",
    "        # Check if the lengths of the outermost arrays are equal\n",
    "        assert len(arr_tt) == len(arr_rn)\n",
    "        # Compare the arrays using the custom function\n",
    "        comparison_result = nan_equal(arr_tt.tolist(), arr_rn.tolist())\n",
    "        # Final assertion\n",
    "        assert comparison_result\n",
    "    except Exception as e:\n",
    "        print(f\"TT array: {arr_tt}\")\n",
    "        print(f\"RN array: {arr_rn}\")\n",
    "        print(f\"Failure limits: {(strt, end)}\")\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"TT:\", arr_tt)\n",
    "    print(\"RN:\", arr_rn)\n",
    "    \n",
    "# key = \"HTXS_Higgs_y\"\n",
    "# key = \"SV_pAngle\"\n",
    "compare_array_region(key, events_tt, events_rn)\n",
    "# print(\"Finished cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c15f7ba-1f90-41b3-8f44-8e6f59b9974c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.203342Z",
     "start_time": "2024-08-01T15:10:40.106928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating trijet mass...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model_TTree_v20' object has no attribute 'Electron'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create the task graph to build a histogram\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating trijet mass...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m reconstructed_top_mass \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_trijet_mass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist_reco_mtop...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m hist_reco_mtop \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mHist\u001b[38;5;241m.\u001b[39mnew\u001b[38;5;241m.\u001b[39mReg(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m375\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$m_\u001b[39m\u001b[38;5;132;01m{bjj}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mDouble()\u001b[38;5;241m.\u001b[39mfill(reconstructed_top_mass)\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mcalculate_trijet_mass\u001b[0;34m(events)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_trijet_mass\u001b[39m(events):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# pT > 30 GeV for leptons, > 25 GeV for jets\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     selected_electrons \u001b[38;5;241m=\u001b[39m \u001b[43mevents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mElectron\u001b[49m[events\u001b[38;5;241m.\u001b[39mElectron\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m&\u001b[39m (np\u001b[38;5;241m.\u001b[39mabs(events\u001b[38;5;241m.\u001b[39mElectron\u001b[38;5;241m.\u001b[39meta) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2.1\u001b[39m)]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected electrons: \u001b[39m\u001b[38;5;124m\"\u001b[39m, selected_electrons)\n\u001b[1;32m      5\u001b[0m     selected_muons \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mMuon[events\u001b[38;5;241m.\u001b[39mMuon\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m&\u001b[39m (np\u001b[38;5;241m.\u001b[39mabs(events\u001b[38;5;241m.\u001b[39mMuon\u001b[38;5;241m.\u001b[39meta) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2.1\u001b[39m)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model_TTree_v20' object has no attribute 'Electron'"
     ]
    }
   ],
   "source": [
    "# create the task graph to build a histogram\n",
    "print(\"Calculating trijet mass...\")\n",
    "reconstructed_top_mass = calculate_trijet_mass(events_list[0])\n",
    "print(\"hist_reco_mtop...\")\n",
    "hist_reco_mtop = hist.dask.Hist.new.Reg(16, 0, 375, label=\"$m_{bjj}$\").Double().fill(reconstructed_top_mass)\n",
    "print(\"Finished cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad62a29",
   "metadata": {},
   "source": [
    "and then once we're ready we can execute the task graph with `.compute()` to get our visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d2d1-7b45-47f6-830d-7c46b479f7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:21:58.923162Z",
     "start_time": "2024-08-01T15:10:40.204505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform computation and visualize\n",
    "artists = hist_reco_mtop.compute().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and annotate the visualization\n",
    "fig_dir = Path.cwd() / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "ax.vlines(175, 0, 10000, colors=[\"grey\"], linestyle=\"dotted\")\n",
    "ax.text(180, 150, \"$m_{t} = 175$ GeV\")\n",
    "ax.set_xlim([0, 375])\n",
    "ax.set_ylim([0, 8000])\n",
    "\n",
    "fig.savefig(fig_dir / \"trijet_mass.png\", dpi=300)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2d6bd-b316-4627-a90c-a8c7e4a028e9",
   "metadata": {},
   "source": [
    "This all matches the (non-Dask) versions of the plots from last summer — see the notebook linked above. Not surprising, but reassuring!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb5a13-a8c0-4236-9c71-7ec6847773cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time for coffea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6684c8",
   "metadata": {},
   "source": [
    "We'll first write the functions to compute the observable and do the histogramming using `awkward-dask` and `hist.dask` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "# perform object selection\n",
    "def object_selection(events):\n",
    "    elecs = events.Electron\n",
    "    muons = events.Muon\n",
    "    jets = events.Jet\n",
    "\n",
    "    electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "    muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                 (muons.pfRelIso04_all < 0.15))\n",
    "    jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "    # Only keep objects that pass our requirements\n",
    "    elecs = elecs[electron_reqs]\n",
    "    muons = muons[muon_reqs]\n",
    "    jets = jets[jet_reqs]\n",
    "\n",
    "    return elecs, muons, jets\n",
    "\n",
    "\n",
    "# event selection for 4j1b and 4j2b\n",
    "def region_selection(elecs, muons, jets):\n",
    "    ######### Store boolean masks with PackedSelection ##########\n",
    "    selections = PackedSelection(dtype='uint64')\n",
    "    # Basic selection criteria\n",
    "    selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "    selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "    selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "    selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    # Complex selection criteria\n",
    "    selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "    selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "    return selections.all(\"4j1b\"), selections.all(\"4j2b\")\n",
    "\n",
    "\n",
    "# observable calculation for 4j2b\n",
    "def calculate_m_reco_top(jets):\n",
    "    # reconstruct hadronic top as bjj system with largest pT\n",
    "    trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2,\n",
    "                                    np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "    return observable\n",
    "\n",
    "\n",
    "# create histograms with observables\n",
    "def create_histograms(events):\n",
    "    hist_4j1b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"HT\", label=r\"$H_T$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    hist_4j2b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "    variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "    process_label = events.metadata[\"process_label\"]  # nicer LaTeX labels\n",
    "\n",
    "    # normalization for MC\n",
    "    x_sec = events.metadata[\"xsec\"]\n",
    "    nevts_total = events.metadata[\"nevts\"]\n",
    "    lumi = 3378 # /pb\n",
    "    if process != \"data\":\n",
    "        xsec_weight = x_sec * lumi / nevts_total\n",
    "    else:\n",
    "        xsec_weight = 1\n",
    "\n",
    "    elecs, muons, jets = object_selection(events)\n",
    "\n",
    "    # region selection\n",
    "    selection_4j1b, selection_4j2b = region_selection(elecs, muons, jets)\n",
    "\n",
    "    # 4j1b: HT\n",
    "    observable_4j1b = ak.sum(jets[selection_4j1b].pt, axis=-1)\n",
    "    hist_4j1b.fill(observable_4j1b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    # 4j2b: m_reco_top\n",
    "    observable_4j2b = calculate_m_reco_top(jets[selection_4j2b])\n",
    "    hist_4j2b.fill(observable_4j2b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    return {\"4j1b\": hist_4j1b, \"4j2b\": hist_4j2b}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3816c2",
   "metadata": {},
   "source": [
    "and prepare the fileset we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b151e-d7a8-49d4-8368-310cbe149b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileset preparation\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "# compared to coffea 0.7: list of file paths becomes list of dicts (path: trename)\n",
    "fileset = utils.file_input.construct_fileset(N_FILES_MAX_PER_SAMPLE)\n",
    "\n",
    "# fileset = {\"ttbar__nominal\": fileset[\"ttbar__nominal\"]}  # to only process nominal ttbar\n",
    "# fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed65cdb",
   "metadata": {},
   "source": [
    "Now we can start using `coffea` with its Dask capabilities. One of the things we need to do is to build the full task graph, which requires looping over all the sample variations (`samples`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc41-0a72-44ec-a8b8-654286a02196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples, _ = dataset_tools.preprocess(fileset, step_size=250_000)\n",
    "\n",
    "# workaround for https://github.com/CoffeaTeam/coffea/issues/1050 (metadata gets dropped, already fixed)\n",
    "for k, v in samples.items():\n",
    "    v[\"metadata\"] = fileset[k][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the task graph\n",
    "tasks = dataset_tools.apply_to_fileset(create_histograms, samples, uproot_options={\"allow_read_errors_with_report\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "and then we can finally execute the full task graph with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "((out, report),) = dask.compute(tasks)  # feels strange that this is a tuple-of-tuple\n",
    "\n",
    "print(f\"total time spent in uproot reading data (or some related metric?): {ak.sum([v['duration'] for v in report.values()]):.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c7fad",
   "metadata": {},
   "source": [
    "To visualize the results, we need to first stack the serperate histograms that were computed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714929f5-9c56-4afa-87e6-5d096af21ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stack all the histograms together (we processed each sample separately)\n",
    "full_histogram_4j1b = sum([v[\"4j1b\"] for v in out.values()])\n",
    "full_histogram_4j2b = sum([v[\"4j2b\"] for v in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c0147-6b4e-4ae7-b4e1-b8eb6b764c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j1b[120j::hist.rebin(2), :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, 1 b-tag\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_1b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba9e07-ec3c-4cdb-be16-4cab17e02a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j2b[:, :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, >= 2 b-tags\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_2b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa67e0-be77-4f70-8a2d-02446ef7793c",
   "metadata": {},
   "source": [
    "This is a plot you can compare to the one in the full AGC notebook — you'll notice they look the same. Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a83e2",
   "metadata": {},
   "source": [
    "If we now investigate the task graph for the nominal $t\\bar{t}$ sample in the optimzied view, which hides from us some of the complexity of the graph we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9195b47-34d0-4947-9691-1563580c3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[0][\"ttbar__nominal\"][\"4j2b\"].visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fca3c-6e3e-42f8-a917-c843822b1f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"100 layers is a large task graph\" on IRIS-HEP Slack, 100 layers happen quickly!\n",
    "for region in [\"4j1b\", \"4j2b\"]:\n",
    "    for process, task in tasks[0].items():\n",
    "        print(f\"{process:>30} {region} {len(task[region].dask.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04c78a-926f-47fb-956c-ef2355213514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns getting read for a given task\n",
    "dak.necessary_columns(tasks[0][\"ttbar__nominal\"][\"4j2b\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
