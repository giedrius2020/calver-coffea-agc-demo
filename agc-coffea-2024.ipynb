{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688000d2-0ab3-4ff8-a4cc-f112422bac42",
   "metadata": {},
   "source": [
    "# AGC + calver coffea on coffea-casa\n",
    "\n",
    "We'll base this on a few sources:\n",
    "- https://github.com/iris-hep/analysis-grand-challenge/tree/main/analyses/cms-open-data-ttbar (AGC, of course)\n",
    "- https://github.com/alexander-held/CompHEP-2023-AGC (contains a simplified version of AGC)\n",
    "- https://github.com/nsmith-/TTGamma_LongExercise/ (credit Nick Smith for helpful examples of the new API)\n",
    "- (and if time allows, weight features: https://github.com/CoffeaTeam/coffea/blob/backports-v0.7.x/binder/accumulators.ipynb / https://coffeateam.github.io/coffea/api/coffea.analysis_tools.Weights.html#coffea.analysis_tools.Weights.partial_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward: 2.6.3\n",
      "dask-awkward: 2024.3.0\n",
      "dask: 2024.7.1\n",
      "uproot: 5.3.10\n",
      "hist: 2.7.2\n",
      "coffea: 2024.6.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "import traceback\n",
    "from dask.distributed import Client\n",
    "import skhep_testdata\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "utils.plotting.set_style()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")\n",
    "\n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"dask-awkward: {dak.__version__}\")\n",
    "print(f\"dask: {dask.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b8cc4-ee4a-4294-86d8-f5db8cf15aaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Produce an AGC histogram with Dask (no coffea yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e667cbdf-9e6f-4c7b-8827-2f6bb35c670b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_trijet_mass(events):\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electron[events.Electron.pt > 30 & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    selected_muons = events.Muon[events.Muon.pt > 30 & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[events.Jet.pt > 25 & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return ak.flatten(trijet_mass)\n",
    "\n",
    "def calculate_trijet_mass_rntuple(events): # TODO: implement\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electron[events.Electron.pt > 30 & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    selected_muons = events.Muon[events.Muon.pt > 30 & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[events.Jet.pt > 25 & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return ak.flatten(trijet_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c972c",
   "metadata": {},
   "source": [
    "Reading in the ROOT file, we can now create a Dask task graph for the calculations and plot that we want to make using `dask-awkward` and `hist.dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27ef364-4245-41f4-8547-ea4b65df0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define work to be done\n",
    "# def uproot_open_materialize(fname):\n",
    "#     BRANCH_LIST = [\n",
    "#         \"GenPart_pt\", \"GenPart_eta\", \"GenPart_phi\", \"CorrT1METJet_phi\",\n",
    "#         \"GenJet_pt\", \"CorrT1METJet_eta\", \"SoftActivityJet_pt\",\n",
    "#         \"Jet_eta\", \"Jet_phi\", \"SoftActivityJet_eta\", \"SoftActivityJet_phi\", \n",
    "#         \"CorrT1METJet_rawPt\", \"Jet_btagDeepFlavB\", \"GenJet_eta\", \n",
    "#         \"GenPart_mass\", \"GenJet_phi\",\n",
    "#         \"Jet_puIdDisc\", \"CorrT1METJet_muonSubtrFactor\", \"Jet_btagDeepFlavCvL\",\n",
    "#         \"Jet_btagDeepFlavQG\", \"Jet_mass\", \"Jet_pt\", \"GenPart_pdgId\",\n",
    "#         \"Jet_btagDeepFlavCvB\", \"Jet_cRegCorr\"\n",
    "#         ]\n",
    "\n",
    "#     filter_name = lambda x: x in BRANCH_LIST\n",
    "\n",
    "#     size_uncompressed = 0\n",
    "#     t0 = time.perf_counter()\n",
    "#     try:\n",
    "#         with uproot.open(fname, filter_name=filter_name) as f:\n",
    "#             num_entries = f[\"Events\"].num_entries\n",
    "#             for b in BRANCH_LIST:\n",
    "#                 f[\"Events\"][b].array()\n",
    "#                 size_uncompressed += f[\"Events\"][b].uncompressed_bytes\n",
    "\n",
    "#             size_read = f.file.source.num_requested_bytes\n",
    "#         exception = None\n",
    "\n",
    "#     except:\n",
    "#         num_entries = 0\n",
    "#         size_read = 0\n",
    "#         size_uncompressed = 0\n",
    "#         exception = traceback.format_exc()\n",
    "\n",
    "#     t1 = time.perf_counter()\n",
    "#     time_finished = datetime.datetime.now()\n",
    "#     return {\"fname\": fname, \"read\": size_read, \"uncompressed\": size_uncompressed, \"num_entries\": num_entries,\n",
    "#             \"runtime\": t1-t0, \"time_finished\": time_finished, \"exception\": exception}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da366d3-1480-4582-9f01-70a03e531da5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttbar_file = \"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/\"\\\n",
    "    \"TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_\"\\\n",
    "    \"mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\" # 533M size\n",
    "\n",
    "# Choose RNTuple file instead:\n",
    "# rntuple_dirs = []\n",
    "# rntuple_files = []\n",
    "# TTbar original: root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root # 533M size\n",
    "# root://eospublic.cern.ch//eos/root-eos/AGC/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\n",
    "# root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\n",
    "\n",
    "\n",
    "\n",
    "# xrd_fpath = \"root://eospublic.cern.ch//eos/root-eos/AGC\"\n",
    "# # Smallest files are chosen, because big ones are crashing in computation step.\n",
    "# data_path_ends = [\n",
    "# \"/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\", # it is 451 MB size,\n",
    "# \"/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\"\n",
    "# ]\n",
    "# for path_end in data_path_ends:\n",
    "#     rntuple_files.append(xrd_fpath + path_end)\n",
    "    \n",
    "# # List all files in each directory\n",
    "# for dir_path in rntuple_dirs:\n",
    "#     print(f\"Getting files from: {dir_path}\")\n",
    "#     list_files_in_directory(dir_path)\n",
    "\n",
    "# Choose files for analysis:\n",
    "# all_files = rntuple_files\n",
    "# all_files.append(\"/home/cms-jovyan/.local/skhepdata/Run2012BC_DoubleMuParked_Muons_rntuple_1000evts.root/scikit-hep-scikit-hep-testdata-9ce1f2b/src/skhep_testdata/data/Run2012BC_DoubleMuParked_Muons_rntuple_1000evts.root\")\n",
    "# print(skhep_testdata.data_path(\"Run2012BC_DoubleMuParked_Muons_rntuple_1000evts.root\"))\n",
    "\n",
    "\n",
    "# File dir:  root://eospublic.cern.ch//eos/root-eos/AGC/eos/root/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/\n",
    "# File dir2:  root://eospublic.cern.ch//eos/root-eos/AGC/eos/root/AGC/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a00374a-83d5-48b8-8e68-7097936170ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was loaded, event count:  947\n",
      "File was loaded, event count:  947\n",
      "File was loaded, event count:  947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if chosen_file == rntuple_files[0]:\n",
    "# try:\n",
    "#     with uproot.open(chosen_file) as f:\n",
    "#         num_entries = f[\"Events\"].num_entries\n",
    "#         events = f[\"Events\"]\n",
    "#         print(\"Events len: \", len(events))\n",
    "#     exception = None\n",
    "# except:\n",
    "#     events = None\n",
    "#     num_entries = 0\n",
    "#     size_read = 0\n",
    "#     size_uncompressed = 0\n",
    "#     exception = traceback.format_exc()\n",
    "#     print(exception)\n",
    "# else: # TTBar file case:\n",
    "# for file in all_files:\n",
    "#     print(\"File to analyze: \", file)\n",
    "    # One way of opening the file (coffea approach):\n",
    "    # print(events.fields)\n",
    "\n",
    "# # TTree file analysis:\n",
    "# events1 = NanoEventsFactory.from_root({all_files[0]: \"Events\"}, schemaclass=NanoAODSchema).events()\n",
    "# selected_electrons = events1.Electron[events1.Electron.pt > 30 & (np.abs(events1.Electron.eta) < 2.1)]\n",
    "# selected_muons = events1.Muon[events1.Muon.pt > 30 & (np.abs(events1.Muon.eta) < 2.1)]\n",
    "# selected_jets = events1.Jet[events1.Jet.pt > 25 & (np.abs(events1.Jet.eta) < 2.4)]\n",
    "\n",
    "# Files are downloaded locally:\n",
    "all_files = []\n",
    "all_files.append(\"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\")\n",
    "all_files.append(\"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\")\n",
    "all_files.append(\"/home/cms-jovyan/my_root_files/rntuple/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\") # RNTuple\n",
    "\n",
    "with uproot.open(all_files[0]) as f: # Remote TTree\n",
    "    events1 = f[\"Events\"]\n",
    "    print(\"File was loaded, event count: \", len(events1.keys()))\n",
    "\n",
    "with uproot.open(all_files[1]) as f:\n",
    "    events2 = f[\"Events\"]\n",
    "    print(\"File was loaded, event count: \", len(events2.keys()))\n",
    "    \n",
    "with uproot.open(all_files[2]) as f:\n",
    "    events3= f[\"Events\"]\n",
    "    print(\"File was loaded, event count: \", len(events3.keys()))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f0b5d9-cd7a-4f86-bca3-a2e8a15f9570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [9.65], [61.6, 25.7, 23.9, 8.55], [79.9], ..., [...], [7.32], [15.8], []]\n",
      "Time for events1.arrays()['Electron_pt']:  43.30151801696047\n",
      "[[], [9.65], [61.6, 25.7, 23.9, 8.55], [79.9], ..., [...], [7.32], [15.8], []]\n",
      "Time for events1['Electron_pt'].array():  0.014032426988705993\n",
      "*****\n",
      "[[], [], [], [], [17.1, 8.75, 5.49], [], ..., [...], [30.6], [], [34.3], [23.8]]\n",
      "Time for events2.arrays()['Electron_pt']:  43.88196036010049\n",
      "[[], [], [], [], [17.1, 8.75, 5.49], [], ..., [...], [30.6], [], [34.3], [23.8]]\n",
      "Time for events2['Electron_pt'].array():  0.01606880221515894\n",
      "*****\n",
      "[[], [9.65], [61.6, 25.7, 23.9, 8.55], ..., [11.6], [86.6], [33.7, 30.9]]\n",
      "Time for events3.arrays()['Electron_pt']:  4.742254454875365\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Model_ROOT_3a3a_Experimental_3a3a_RNTuple' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m time_3 \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mtimeit(array_3, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime for events3.arrays()[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElectron_pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;124m\"\u001b[39m, time_3)\n\u001b[0;32m---> 39\u001b[0m time_3_direct \u001b[38;5;241m=\u001b[39m \u001b[43mtimeit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_3_direct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime for events3[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElectron_pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m].array(): \u001b[39m\u001b[38;5;124m\"\u001b[39m, time_3_direct)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/timeit.py:234\u001b[0m, in \u001b[0;36mtimeit\u001b[0;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimeit\u001b[39m(stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, timer\u001b[38;5;241m=\u001b[39mdefault_timer,\n\u001b[1;32m    232\u001b[0m            number\u001b[38;5;241m=\u001b[39mdefault_number, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTimer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/timeit.py:178\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    176\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer, _stmt)\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36marray_3_direct\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_3_direct\u001b[39m():\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mevents3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mElectron_pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39marray())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Model_ROOT_3a3a_Experimental_3a3a_RNTuple' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "## Test times for each function:\n",
    "\n",
    "# import timeit\n",
    "\n",
    "# # Define the operations as functions\n",
    "# def array_1():\n",
    "#     print(events1.arrays()[\"Electron_pt\"])\n",
    "\n",
    "# def array_1_direct():\n",
    "#     print(events1[\"Electron_pt\"].array())\n",
    "\n",
    "# def array_2():\n",
    "#     print(events2.arrays()[\"Electron_pt\"])\n",
    "\n",
    "# def array_2_direct():\n",
    "#     print(events2[\"Electron_pt\"].array())\n",
    "\n",
    "# def array_3():\n",
    "#     print(events3.arrays()[\"Electron_pt\"])\n",
    "\n",
    "# def array_3_direct():\n",
    "#     print(events3[\"Electron_pt\"].array())\n",
    "\n",
    "# # Time the operations\n",
    "# time_1 = timeit.timeit(array_1, number=1)\n",
    "# print(\"Time for events1.arrays()['Electron_pt']: \", time_1)\n",
    "# time_1_direct = timeit.timeit(array_1_direct, number=1)\n",
    "# print(\"Time for events1['Electron_pt'].array(): \", time_1_direct)\n",
    "\n",
    "# print(\"*****\")\n",
    "\n",
    "# time_2 = timeit.timeit(array_2, number=1)\n",
    "# print(\"Time for events2.arrays()['Electron_pt']: \", time_2)\n",
    "# time_2_direct = timeit.timeit(array_2_direct, number=1)\n",
    "# print(\"Time for events2['Electron_pt'].array(): \", time_2_direct)\n",
    "\n",
    "# print(\"*****\")\n",
    "\n",
    "# time_3 = timeit.timeit(array_3, number=1)\n",
    "# print(\"Time for events3.arrays()['Electron_pt']: \", time_3)\n",
    "# time_3_direct = timeit.timeit(array_3_direct, number=1)\n",
    "# print(\"Time for events3['Electron_pt'].array(): \", time_3_direct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bacbf98-8308-4964-a453-49671ef79bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [9.65], [61.6, 25.7, 23.9, 8.55], ..., [11.6], [86.6], [33.7, 30.9]]\n"
     ]
    }
   ],
   "source": [
    "print(events3.arrays()['Electron_pt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15f7ba-1f90-41b3-8f44-8e6f59b9974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the task graph to build a histogram\n",
    "print(\"Calculating trijet mass...\")\n",
    "reconstructed_top_mass = calculate_trijet_mass(events)\n",
    "print(\"hist_reco_mtop...\")\n",
    "hist_reco_mtop = hist.dask.Hist.new.Reg(16, 0, 375, label=\"$m_{bjj}$\").Double().fill(reconstructed_top_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad62a29",
   "metadata": {},
   "source": [
    "and then once we're ready we can execute the task graph with `.compute()` to get our visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d2d1-7b45-47f6-830d-7c46b479f7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform computation and visualize\n",
    "artists = hist_reco_mtop.compute().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and annotate the visualization\n",
    "fig_dir = Path.cwd() / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "ax.vlines(175, 0, 10000, colors=[\"grey\"], linestyle=\"dotted\")\n",
    "ax.text(180, 150, \"$m_{t} = 175$ GeV\")\n",
    "ax.set_xlim([0, 375])\n",
    "ax.set_ylim([0, 8000])\n",
    "\n",
    "fig.savefig(fig_dir / \"trijet_mass.png\", dpi=300)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2d6bd-b316-4627-a90c-a8c7e4a028e9",
   "metadata": {},
   "source": [
    "This all matches the (non-Dask) versions of the plots from last summer — see the notebook linked above. Not surprising, but reassuring!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb5a13-a8c0-4236-9c71-7ec6847773cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time for coffea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6684c8",
   "metadata": {},
   "source": [
    "We'll first write the functions to compute the observable and do the histogramming using `awkward-dask` and `hist.dask` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "# perform object selection\n",
    "def object_selection(events):\n",
    "    elecs = events.Electron\n",
    "    muons = events.Muon\n",
    "    jets = events.Jet\n",
    "\n",
    "    electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "    muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                 (muons.pfRelIso04_all < 0.15))\n",
    "    jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "    # Only keep objects that pass our requirements\n",
    "    elecs = elecs[electron_reqs]\n",
    "    muons = muons[muon_reqs]\n",
    "    jets = jets[jet_reqs]\n",
    "\n",
    "    return elecs, muons, jets\n",
    "\n",
    "\n",
    "# event selection for 4j1b and 4j2b\n",
    "def region_selection(elecs, muons, jets):\n",
    "    ######### Store boolean masks with PackedSelection ##########\n",
    "    selections = PackedSelection(dtype='uint64')\n",
    "    # Basic selection criteria\n",
    "    selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "    selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "    selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "    selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    # Complex selection criteria\n",
    "    selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "    selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "    return selections.all(\"4j1b\"), selections.all(\"4j2b\")\n",
    "\n",
    "\n",
    "# observable calculation for 4j2b\n",
    "def calculate_m_reco_top(jets):\n",
    "    # reconstruct hadronic top as bjj system with largest pT\n",
    "    trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2,\n",
    "                                    np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "    return observable\n",
    "\n",
    "\n",
    "# create histograms with observables\n",
    "def create_histograms(events):\n",
    "    hist_4j1b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"HT\", label=r\"$H_T$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    hist_4j2b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "    variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "    process_label = events.metadata[\"process_label\"]  # nicer LaTeX labels\n",
    "\n",
    "    # normalization for MC\n",
    "    x_sec = events.metadata[\"xsec\"]\n",
    "    nevts_total = events.metadata[\"nevts\"]\n",
    "    lumi = 3378 # /pb\n",
    "    if process != \"data\":\n",
    "        xsec_weight = x_sec * lumi / nevts_total\n",
    "    else:\n",
    "        xsec_weight = 1\n",
    "\n",
    "    elecs, muons, jets = object_selection(events)\n",
    "\n",
    "    # region selection\n",
    "    selection_4j1b, selection_4j2b = region_selection(elecs, muons, jets)\n",
    "\n",
    "    # 4j1b: HT\n",
    "    observable_4j1b = ak.sum(jets[selection_4j1b].pt, axis=-1)\n",
    "    hist_4j1b.fill(observable_4j1b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    # 4j2b: m_reco_top\n",
    "    observable_4j2b = calculate_m_reco_top(jets[selection_4j2b])\n",
    "    hist_4j2b.fill(observable_4j2b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    return {\"4j1b\": hist_4j1b, \"4j2b\": hist_4j2b}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3816c2",
   "metadata": {},
   "source": [
    "and prepare the fileset we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b151e-d7a8-49d4-8368-310cbe149b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileset preparation\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "# compared to coffea 0.7: list of file paths becomes list of dicts (path: trename)\n",
    "fileset = utils.file_input.construct_fileset(N_FILES_MAX_PER_SAMPLE)\n",
    "\n",
    "# fileset = {\"ttbar__nominal\": fileset[\"ttbar__nominal\"]}  # to only process nominal ttbar\n",
    "# fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed65cdb",
   "metadata": {},
   "source": [
    "Now we can start using `coffea` with its Dask capabilities. One of the things we need to do is to build the full task graph, which requires looping over all the sample variations (`samples`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc41-0a72-44ec-a8b8-654286a02196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples, _ = dataset_tools.preprocess(fileset, step_size=250_000)\n",
    "\n",
    "# workaround for https://github.com/CoffeaTeam/coffea/issues/1050 (metadata gets dropped, already fixed)\n",
    "for k, v in samples.items():\n",
    "    v[\"metadata\"] = fileset[k][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the task graph\n",
    "tasks = dataset_tools.apply_to_fileset(create_histograms, samples, uproot_options={\"allow_read_errors_with_report\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "and then we can finally execute the full task graph with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "((out, report),) = dask.compute(tasks)  # feels strange that this is a tuple-of-tuple\n",
    "\n",
    "print(f\"total time spent in uproot reading data (or some related metric?): {ak.sum([v['duration'] for v in report.values()]):.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c7fad",
   "metadata": {},
   "source": [
    "To visualize the results, we need to first stack the serperate histograms that were computed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714929f5-9c56-4afa-87e6-5d096af21ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stack all the histograms together (we processed each sample separately)\n",
    "full_histogram_4j1b = sum([v[\"4j1b\"] for v in out.values()])\n",
    "full_histogram_4j2b = sum([v[\"4j2b\"] for v in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c0147-6b4e-4ae7-b4e1-b8eb6b764c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j1b[120j::hist.rebin(2), :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, 1 b-tag\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_1b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba9e07-ec3c-4cdb-be16-4cab17e02a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j2b[:, :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, >= 2 b-tags\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_2b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa67e0-be77-4f70-8a2d-02446ef7793c",
   "metadata": {},
   "source": [
    "This is a plot you can compare to the one in the full AGC notebook — you'll notice they look the same. Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a83e2",
   "metadata": {},
   "source": [
    "If we now investigate the task graph for the nominal $t\\bar{t}$ sample in the optimzied view, which hides from us some of the complexity of the graph we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9195b47-34d0-4947-9691-1563580c3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[0][\"ttbar__nominal\"][\"4j2b\"].visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fca3c-6e3e-42f8-a917-c843822b1f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"100 layers is a large task graph\" on IRIS-HEP Slack, 100 layers happen quickly!\n",
    "for region in [\"4j1b\", \"4j2b\"]:\n",
    "    for process, task in tasks[0].items():\n",
    "        print(f\"{process:>30} {region} {len(task[region].dask.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04c78a-926f-47fb-956c-ef2355213514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns getting read for a given task\n",
    "dak.necessary_columns(tasks[0][\"ttbar__nominal\"][\"4j2b\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
