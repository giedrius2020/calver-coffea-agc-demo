{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688000d2-0ab3-4ff8-a4cc-f112422bac42",
   "metadata": {},
   "source": [
    "# AGC + calver coffea on coffea-casa\n",
    "\n",
    "We'll base this on a few sources:\n",
    "- https://github.com/iris-hep/analysis-grand-challenge/tree/main/analyses/cms-open-data-ttbar (AGC, of course)\n",
    "- https://github.com/alexander-held/CompHEP-2023-AGC (contains a simplified version of AGC)\n",
    "- https://github.com/nsmith-/TTGamma_LongExercise/ (credit Nick Smith for helpful examples of the new API)\n",
    "- (and if time allows, weight features: https://github.com/CoffeaTeam/coffea/blob/backports-v0.7.x/binder/accumulators.ipynb / https://coffeateam.github.io/coffea/api/coffea.analysis_tools.Weights.html#coffea.analysis_tools.Weights.partial_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:38.782002Z",
     "start_time": "2024-08-01T15:10:37.422997Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward: 2.6.3\n",
      "dask-awkward: 2024.3.0\n",
      "dask: 2024.4.0\n",
      "uproot: 5.3.11.dev3+g2a20562\n",
      "hist: 2.7.2\n",
      "coffea: 2024.6.2.dev12+ge7666484\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "import traceback\n",
    "from dask.distributed import Client\n",
    "import skhep_testdata\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "utils.plotting.set_style()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")\n",
    "\n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"dask-awkward: {dak.__version__}\")\n",
    "print(f\"dask: {dask.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b8cc4-ee4a-4294-86d8-f5db8cf15aaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Produce an AGC histogram with Dask (no coffea yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e667cbdf-9e6f-4c7b-8827-2f6bb35c670b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:38.792459Z",
     "start_time": "2024-08-01T15:10:38.783403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_trijet_mass(events):\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electron[events.Electron.pt > 30 & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    print(\"Selected electrons: \", selected_electrons)\n",
    "    selected_muons = events.Muon[events.Muon.pt > 30 & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[events.Jet.pt > 25 & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return ak.flatten(trijet_mass)\n",
    "\n",
    "def calculate_trijet_mass_rntuple(events): # TODO: implement\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    events.arrays([\"Electron\"])[\"Electron\"]\n",
    "    selected_electrons = events.arrays([\"Electron\"])[events.Electron.pt > 30 & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    selected_muons = events.Muon[events.Muon.pt > 30 & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[events.Jet.pt > 25 & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return ak.flatten(trijet_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c972c",
   "metadata": {},
   "source": [
    "Reading in the ROOT file, we can now create a Dask task graph for the calculations and plot that we want to make using `dask-awkward` and `hist.dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a00374a-83d5-48b8-8e68-7097936170ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.083914Z",
     "start_time": "2024-08-01T15:10:38.838844Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tree = uproot.open(file, **uproot_options):  <class 'uproot.models.TTree.Model_TTree_v20'>\n",
      "File was loaded, fields count:  62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if chosen_file == rntuple_files[0]:\n",
    "# try:\n",
    "#     with uproot.open(chosen_file) as f:\n",
    "#         num_entries = f[\"Events\"].num_entries\n",
    "#         events = f[\"Events\"]\n",
    "#         print(\"Events len: \", len(events))\n",
    "#     exception = None\n",
    "# except:\n",
    "#     events = None\n",
    "#     num_entries = 0\n",
    "#     size_read = 0\n",
    "#     size_uncompressed = 0\n",
    "#     exception = traceback.format_exc()\n",
    "#     print(exception)\n",
    "# else: # TTBar file case:\n",
    "# for file in all_files:\n",
    "#     print(\"File to analyze: \", file)\n",
    "    # One way of opening the file (coffea approach):\n",
    "    # print(events.fields)\n",
    "\n",
    "# # TTree file analysis:\n",
    "# events1 = NanoEventsFactory.from_root({all_files[0]: \"Events\"}, schemaclass=NanoAODSchema).events()\n",
    "# selected_electrons = events1.Electron[events1.Electron.pt > 30 & (np.abs(events1.Electron.eta) < 2.1)]\n",
    "# selected_muons = events1.Muon[events1.Muon.pt > 30 & (np.abs(events1.Muon.eta) < 2.1)]\n",
    "# selected_jets = events1.Jet[events1.Jet.pt > 25 & (np.abs(events1.Jet.eta) < 2.4)]\n",
    "\n",
    "all_files = []\n",
    "events_list = []\n",
    "\n",
    "# Some files are downloaded locally:\n",
    "# all_files.append(ttbar_file)\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\") # 533M size\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\") # RNTuple remote\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\")\n",
    "all_files.append(\"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\") # TTree ttbar original\n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\") # TTree local\n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/rntuple/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\")  # RNTuple local\n",
    "events_list = []\n",
    "\n",
    "\n",
    "def load_files_with_uproot(files):\n",
    "    for fl in files:\n",
    "        with uproot.open(fl) as f:\n",
    "            events = f[\"Events\"]\n",
    "            events_list.append(events)\n",
    "            print(\"File was loaded, event count: \", len(events.keys()))\n",
    "            \n",
    "            # NOTE: to access array: # events.arrays([\"Electron_pt\"])[\"Electron_pt\"]\n",
    "    return events_list\n",
    "        \n",
    "def load_files_with_coffea(files):\n",
    "    for fl in files:\n",
    "        events = NanoEventsFactory.from_root({fl: \"Events\"}, schemaclass=NanoAODSchema, delayed=False).events()\n",
    "        events_list.append(events)\n",
    "        print(\"File was loaded, fields count: \", len(events.fields))\n",
    "        \n",
    "    return events_list\n",
    "\n",
    "# events_list_uproot_approach = load_files_with_uproot(all_files)\n",
    "# print(\"Num entries: \", events_list_uproot_approach[1].num_entries)\n",
    "\n",
    "events_list = load_files_with_coffea(all_files)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58212611-abcb-42ed-96fc-9ad0a0e61d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cell...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "no field named '_related_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m      4\u001b[0m file \u001b[38;5;241m=\u001b[39m all_files[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(\"Name: \", events.name)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(\"num_entries: \", events.num_entries)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(\"Show(): \", events.show())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"_column_records_dict: \", events._column_records_dict)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(\"header: \", events.header)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_related_ids: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mevents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_related_ids\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# array = events.arrays(filter_names=[\"Electron_pt\", \"Electron_eta\"])[:20]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"events.arrays(): \", events.arrays())\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(\"----------\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# for val in events_list[1].num_entries(): # first 5\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     print(\"Key: \", val)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/awkward/highlevel.py:1239\u001b[0m, in \u001b[0;36mArray.__getattr__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile trying to get field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, an exception \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurred:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(err)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1237\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno field named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: no field named '_related_ids'"
     ]
    }
   ],
   "source": [
    "print(\"Starting cell...\")\n",
    "\n",
    "events = events_list[0]\n",
    "file = all_files[0]\n",
    "\n",
    "\n",
    "## Various available properties:\n",
    "# print(\"Name: \", events.name)\n",
    "# print(\"num_entries: \", events.num_entries)\n",
    "# print(\"Show(): \", events.show())\n",
    "# print(\"header: \", events.header)\n",
    "# print(\"footer: \", events.footer)\n",
    "# print(\"field_names: \", events.field_names[:10])\n",
    "# print(\"column_records: \", events.column_records[:10])\n",
    "# print(\"keys: \", events.keys()[:10])\n",
    "# print(\"_column_records_dict: \", events._column_records_dict)\n",
    "# print(\"_related_ids: \", events._related_ids)\n",
    "# print(\"page_list_envelopes: \", events.page_list_envelopes)\n",
    "# print(\"keys: \", events.keys())\n",
    "\n",
    "\n",
    "## Experimenting with array access:\n",
    "# array = events.arrays(filter_names=[\"Electron_pt\", \"Electron_eta\"])[:20]\n",
    "# print(\"events.arrays(): \", events.arrays())\n",
    "# print(\"----------\")\n",
    "# print(\"events.arrays().show(): \", events.arrays().show())\n",
    "# print(\"----------\")\n",
    "# tree = uproot.open(file)\n",
    "\n",
    "# for key in  events.keys(): \n",
    "#     # branch = events.arrays([key])[key]\n",
    "#     print(key)\n",
    "\n",
    "\n",
    "## Things that do not work:\n",
    "# for key, branch in events.iteritems(): # Does not work with RNTuple. AttributeError: no field named 'iteritems'. However, it does work with TTree.\n",
    "# for key, branch in events.arrays().iteritems(): # Does not work both with TTree or RNTuple.\n",
    "# print(list(events.arrays().keys())) # Does not work. No field names keys()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0b5d9-cd7a-4f86-bca3-a2e8a15f9570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.088275Z",
     "start_time": "2024-08-01T15:10:40.085122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test times for each function:\n",
    "\n",
    "# import timeit\n",
    "\n",
    "# # Define the operations as functions\n",
    "# def array_1():\n",
    "#     print(events_list[0].arrays([\"Electron_pt\"])[\"Electron_pt\"])\n",
    "\n",
    "# def array_1_direct():\n",
    "#     print(events_list[0][\"Electron_pt\"].array())\n",
    "\n",
    "# def array_2():\n",
    "#     print(events_list[1].arrays([\"Electron_pt\"])[\"Electron_pt\"])\n",
    "\n",
    "# def array_2_direct():\n",
    "#     print(events_list[1][\"Electron_pt\"].array())\n",
    "\n",
    "# def array_3():\n",
    "#     print(events_list[2].arrays([\"Electron_pt\"])[\"Electron_pt\"])\n",
    "\n",
    "# def array_3_direct():\n",
    "#     print(events_list[2][\"Electron_pt\"].array())\n",
    "\n",
    "# # Time the operations\n",
    "# time_1 = timeit.timeit(array_1, number=1)\n",
    "# print(\"Time for events1.arrays(['Electron_pt'])['Electron_pt']: \", time_1)\n",
    "# time_1_direct = timeit.timeit(array_1_direct, number=1)\n",
    "# print(\"Time for events1['Electron_pt'].array(): \", time_1_direct)\n",
    "\n",
    "# print(\"*****\")\n",
    "\n",
    "# time_2 = timeit.timeit(array_2, number=1)\n",
    "# print(\"Time for events2.arrays(['Electron_pt'])['Electron_pt']: \", time_2)\n",
    "# time_2_direct = timeit.timeit(array_2_direct, number=1)\n",
    "# print(\"Time for events2['Electron_pt'].array(): \", time_2_direct)\n",
    "\n",
    "# print(\"*****\")\n",
    "\n",
    "# time_3 = timeit.timeit(array_3, number=1)\n",
    "# print(\"Time for events3.arrays(['Electron_pt'])['Electron_pt']: \", time_3)\n",
    "# time_3_direct = timeit.timeit(array_3_direct, number=1)\n",
    "# print(\"Time for events3['Electron_pt'].array(): \", time_3_direct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacbf98-8308-4964-a453-49671ef79bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.106276Z",
     "start_time": "2024-08-01T15:10:40.089030Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15f7ba-1f90-41b3-8f44-8e6f59b9974c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.203342Z",
     "start_time": "2024-08-01T15:10:40.106928Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the task graph to build a histogram\n",
    "print(\"Calculating trijet mass...\")\n",
    "reconstructed_top_mass = calculate_trijet_mass(events_list[0])\n",
    "print(\"hist_reco_mtop...\")\n",
    "hist_reco_mtop = hist.dask.Hist.new.Reg(16, 0, 375, label=\"$m_{bjj}$\").Double().fill(reconstructed_top_mass)\n",
    "print(\"Finished cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad62a29",
   "metadata": {},
   "source": [
    "and then once we're ready we can execute the task graph with `.compute()` to get our visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d2d1-7b45-47f6-830d-7c46b479f7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:21:58.923162Z",
     "start_time": "2024-08-01T15:10:40.204505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform computation and visualize\n",
    "artists = hist_reco_mtop.compute().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and annotate the visualization\n",
    "fig_dir = Path.cwd() / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "ax.vlines(175, 0, 10000, colors=[\"grey\"], linestyle=\"dotted\")\n",
    "ax.text(180, 150, \"$m_{t} = 175$ GeV\")\n",
    "ax.set_xlim([0, 375])\n",
    "ax.set_ylim([0, 8000])\n",
    "\n",
    "fig.savefig(fig_dir / \"trijet_mass.png\", dpi=300)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2d6bd-b316-4627-a90c-a8c7e4a028e9",
   "metadata": {},
   "source": [
    "This all matches the (non-Dask) versions of the plots from last summer — see the notebook linked above. Not surprising, but reassuring!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb5a13-a8c0-4236-9c71-7ec6847773cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time for coffea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6684c8",
   "metadata": {},
   "source": [
    "We'll first write the functions to compute the observable and do the histogramming using `awkward-dask` and `hist.dask` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "# perform object selection\n",
    "def object_selection(events):\n",
    "    elecs = events.Electron\n",
    "    muons = events.Muon\n",
    "    jets = events.Jet\n",
    "\n",
    "    electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "    muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                 (muons.pfRelIso04_all < 0.15))\n",
    "    jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "    # Only keep objects that pass our requirements\n",
    "    elecs = elecs[electron_reqs]\n",
    "    muons = muons[muon_reqs]\n",
    "    jets = jets[jet_reqs]\n",
    "\n",
    "    return elecs, muons, jets\n",
    "\n",
    "\n",
    "# event selection for 4j1b and 4j2b\n",
    "def region_selection(elecs, muons, jets):\n",
    "    ######### Store boolean masks with PackedSelection ##########\n",
    "    selections = PackedSelection(dtype='uint64')\n",
    "    # Basic selection criteria\n",
    "    selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "    selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "    selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "    selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    # Complex selection criteria\n",
    "    selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "    selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "    return selections.all(\"4j1b\"), selections.all(\"4j2b\")\n",
    "\n",
    "\n",
    "# observable calculation for 4j2b\n",
    "def calculate_m_reco_top(jets):\n",
    "    # reconstruct hadronic top as bjj system with largest pT\n",
    "    trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2,\n",
    "                                    np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "    return observable\n",
    "\n",
    "\n",
    "# create histograms with observables\n",
    "def create_histograms(events):\n",
    "    hist_4j1b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"HT\", label=r\"$H_T$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    hist_4j2b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "    variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "    process_label = events.metadata[\"process_label\"]  # nicer LaTeX labels\n",
    "\n",
    "    # normalization for MC\n",
    "    x_sec = events.metadata[\"xsec\"]\n",
    "    nevts_total = events.metadata[\"nevts\"]\n",
    "    lumi = 3378 # /pb\n",
    "    if process != \"data\":\n",
    "        xsec_weight = x_sec * lumi / nevts_total\n",
    "    else:\n",
    "        xsec_weight = 1\n",
    "\n",
    "    elecs, muons, jets = object_selection(events)\n",
    "\n",
    "    # region selection\n",
    "    selection_4j1b, selection_4j2b = region_selection(elecs, muons, jets)\n",
    "\n",
    "    # 4j1b: HT\n",
    "    observable_4j1b = ak.sum(jets[selection_4j1b].pt, axis=-1)\n",
    "    hist_4j1b.fill(observable_4j1b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    # 4j2b: m_reco_top\n",
    "    observable_4j2b = calculate_m_reco_top(jets[selection_4j2b])\n",
    "    hist_4j2b.fill(observable_4j2b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    return {\"4j1b\": hist_4j1b, \"4j2b\": hist_4j2b}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3816c2",
   "metadata": {},
   "source": [
    "and prepare the fileset we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b151e-d7a8-49d4-8368-310cbe149b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileset preparation\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "# compared to coffea 0.7: list of file paths becomes list of dicts (path: trename)\n",
    "fileset = utils.file_input.construct_fileset(N_FILES_MAX_PER_SAMPLE)\n",
    "\n",
    "# fileset = {\"ttbar__nominal\": fileset[\"ttbar__nominal\"]}  # to only process nominal ttbar\n",
    "# fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed65cdb",
   "metadata": {},
   "source": [
    "Now we can start using `coffea` with its Dask capabilities. One of the things we need to do is to build the full task graph, which requires looping over all the sample variations (`samples`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc41-0a72-44ec-a8b8-654286a02196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples, _ = dataset_tools.preprocess(fileset, step_size=250_000)\n",
    "\n",
    "# workaround for https://github.com/CoffeaTeam/coffea/issues/1050 (metadata gets dropped, already fixed)\n",
    "for k, v in samples.items():\n",
    "    v[\"metadata\"] = fileset[k][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the task graph\n",
    "tasks = dataset_tools.apply_to_fileset(create_histograms, samples, uproot_options={\"allow_read_errors_with_report\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "and then we can finally execute the full task graph with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "((out, report),) = dask.compute(tasks)  # feels strange that this is a tuple-of-tuple\n",
    "\n",
    "print(f\"total time spent in uproot reading data (or some related metric?): {ak.sum([v['duration'] for v in report.values()]):.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c7fad",
   "metadata": {},
   "source": [
    "To visualize the results, we need to first stack the serperate histograms that were computed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714929f5-9c56-4afa-87e6-5d096af21ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stack all the histograms together (we processed each sample separately)\n",
    "full_histogram_4j1b = sum([v[\"4j1b\"] for v in out.values()])\n",
    "full_histogram_4j2b = sum([v[\"4j2b\"] for v in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c0147-6b4e-4ae7-b4e1-b8eb6b764c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j1b[120j::hist.rebin(2), :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, 1 b-tag\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_1b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba9e07-ec3c-4cdb-be16-4cab17e02a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j2b[:, :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, >= 2 b-tags\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_2b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa67e0-be77-4f70-8a2d-02446ef7793c",
   "metadata": {},
   "source": [
    "This is a plot you can compare to the one in the full AGC notebook — you'll notice they look the same. Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a83e2",
   "metadata": {},
   "source": [
    "If we now investigate the task graph for the nominal $t\\bar{t}$ sample in the optimzied view, which hides from us some of the complexity of the graph we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9195b47-34d0-4947-9691-1563580c3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[0][\"ttbar__nominal\"][\"4j2b\"].visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fca3c-6e3e-42f8-a917-c843822b1f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"100 layers is a large task graph\" on IRIS-HEP Slack, 100 layers happen quickly!\n",
    "for region in [\"4j1b\", \"4j2b\"]:\n",
    "    for process, task in tasks[0].items():\n",
    "        print(f\"{process:>30} {region} {len(task[region].dask.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04c78a-926f-47fb-956c-ef2355213514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns getting read for a given task\n",
    "dak.necessary_columns(tasks[0][\"ttbar__nominal\"][\"4j2b\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
