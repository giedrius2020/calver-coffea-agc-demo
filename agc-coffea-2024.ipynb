{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688000d2-0ab3-4ff8-a4cc-f112422bac42",
   "metadata": {},
   "source": [
    "# AGC + calver coffea on coffea-casa\n",
    "\n",
    "We'll base this on a few sources:\n",
    "- https://github.com/iris-hep/analysis-grand-challenge/tree/main/analyses/cms-open-data-ttbar (AGC, of course)\n",
    "- https://github.com/alexander-held/CompHEP-2023-AGC (contains a simplified version of AGC)\n",
    "- https://github.com/nsmith-/TTGamma_LongExercise/ (credit Nick Smith for helpful examples of the new API)\n",
    "- (and if time allows, weight features: https://github.com/CoffeaTeam/coffea/blob/backports-v0.7.x/binder/accumulators.ipynb / https://coffeateam.github.io/coffea/api/coffea.analysis_tools.Weights.html#coffea.analysis_tools.Weights.partial_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:38.782002Z",
     "start_time": "2024-08-01T15:10:37.422997Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward: 2.6.3\n",
      "dask-awkward: 2024.3.0\n",
      "dask: 2024.4.0\n",
      "uproot: 5.3.11.dev3+g2a20562\n",
      "hist: 2.7.2\n",
      "coffea: 2024.6.2.dev12+ge7666484\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "import traceback\n",
    "from dask.distributed import Client\n",
    "import skhep_testdata\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "utils.plotting.set_style()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")\n",
    "\n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"dask-awkward: {dak.__version__}\")\n",
    "print(f\"dask: {dask.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b8cc4-ee4a-4294-86d8-f5db8cf15aaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Produce an AGC histogram with Dask (no coffea yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e667cbdf-9e6f-4c7b-8827-2f6bb35c670b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:38.792459Z",
     "start_time": "2024-08-01T15:10:38.783403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_trijet_mass(events):\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electron[events.Electron.pt > 30 & (np.abs(events.Electron.eta) < 2.1)]\n",
    "    print(\"Selected electrons: \", selected_electrons)\n",
    "    selected_muons = events.Muon[events.Muon.pt > 30 & (np.abs(events.Muon.eta) < 2.1)]\n",
    "    selected_jets = events.Jet[events.Jet.pt > 25 & (np.abs(events.Jet.eta) < 2.4)]\n",
    "\n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 0.5\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "\n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "\n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    return ak.flatten(trijet_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c972c",
   "metadata": {},
   "source": [
    "Reading in the ROOT file, we can now create a Dask task graph for the calculations and plot that we want to make using `dask-awkward` and `hist.dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a00374a-83d5-48b8-8e68-7097936170ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.083914Z",
     "start_time": "2024-08-01T15:10:38.838844Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tree = uproot.open(file, **uproot_options):  <class 'uproot.models.TTree.Model_TTree_v20'>\n",
      "coffea/nanoevents/mapping/uproot.py:\n",
      "tree.title:\n",
      "Events\n",
      "tree.contents:\n",
      "[{'class': 'NumpyArray', 'primitive': 'uint32', 'inner_shape': [], 'parameters': {'__doc__': 'run/i'}, 'form_key': 'run%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'uint32', 'inner_shape': [], 'parameters': {'__doc__': 'luminosityBlock/i'}, 'form_key': 'luminosityBlock%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'uint64', 'inner_shape': [], 'parameters': {'__doc__': 'event/l'}, 'form_key': 'event%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'pt of the Higgs boson as identified in HTXS'}, 'form_key': 'HTXS_Higgs_pt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'rapidity of the Higgs boson as identified in HTXS'}, 'form_key': 'HTXS_Higgs_y%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.1 category(jet pt>25 GeV)'}, 'form_key': 'HTXS_stage1_1_cat_pTjet25GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.1 category(jet pt>30 GeV)'}, 'form_key': 'HTXS_stage1_1_cat_pTjet30GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.1-fine category(jet pt>25 GeV)'}, 'form_key': 'HTXS_stage1_1_fine_cat_pTjet25GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.1-fine category(jet pt>30 GeV)'}, 'form_key': 'HTXS_stage1_1_fine_cat_pTjet30GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.2 category(jet pt>25 GeV)'}, 'form_key': 'HTXS_stage1_2_cat_pTjet25GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.2 category(jet pt>30 GeV)'}, 'form_key': 'HTXS_stage1_2_cat_pTjet30GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.2-fine category(jet pt>25 GeV)'}, 'form_key': 'HTXS_stage1_2_fine_cat_pTjet25GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1.2-fine category(jet pt>30 GeV)'}, 'form_key': 'HTXS_stage1_2_fine_cat_pTjet30GeV%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-0 category'}, 'form_key': 'HTXS_stage_0%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1 category (jet pt>25 GeV)'}, 'form_key': 'HTXS_stage_1_pTjet25%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'int32', 'inner_shape': [], 'parameters': {'__doc__': 'HTXS stage-1 category (jet pt>30 GeV)'}, 'form_key': 'HTXS_stage_1_pTjet30%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'uint8', 'inner_shape': [], 'parameters': {'__doc__': 'number of jets with pt>25 GeV as identified in HTXS'}, 'form_key': 'HTXS_njets25%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'uint8', 'inner_shape': [], 'parameters': {'__doc__': 'number of jets with pt>30 GeV as identified in HTXS'}, 'form_key': 'HTXS_njets30%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'b-tag event weight for CSVV2'}, 'form_key': 'btagWeight_CSVV2%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'b-tag event weight for DeepCSVB'}, 'form_key': 'btagWeight_DeepCSVB%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'phi'}, 'form_key': 'CaloMET_phi%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'pt'}, 'form_key': 'CaloMET_pt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'scalar sum of Et'}, 'form_key': 'CaloMET_sumEt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'raw chs PF MET phi'}, 'form_key': 'ChsMET_phi%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'raw chs PF MET pt'}, 'form_key': 'ChsMET_pt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'raw chs PF scalar sum of Et'}, 'form_key': 'ChsMET_sumEt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'uint32', 'inner_shape': [], 'parameters': {'__doc__': 'Additional low-pt jets for Type-1 MET re-correction'}, 'form_key': 'nCorrT1METJet%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'jet catchment area, for JECs'}, 'form_key': 'CorrT1METJet_area%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'CorrT1METJet_area%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'eta'}, 'form_key': 'CorrT1METJet_eta%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'CorrT1METJet_eta%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': '1-(muon-subtracted raw pt)/(raw pt)'}, 'form_key': 'CorrT1METJet_muonSubtrFactor%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'CorrT1METJet_muonSubtrFactor%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'phi'}, 'form_key': 'CorrT1METJet_phi%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'CorrT1METJet_phi%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': \"pt()*jecFactor('Uncorrected')\"}, 'form_key': 'CorrT1METJet_rawPt%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'CorrT1METJet_rawPt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'DeepmET ResolutionTune phi'}, 'form_key': 'DeepMETResolutionTune_phi%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'DeepMET ResolutionTune pt'}, 'form_key': 'DeepMETResolutionTune_pt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'DeepMET ResponseTune phi'}, 'form_key': 'DeepMETResponseTune_phi%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'DeepMET ResponseTune pt'}, 'form_key': 'DeepMETResponseTune_pt%2C%21load'}, {'class': 'NumpyArray', 'primitive': 'uint32', 'inner_shape': [], 'parameters': {'__doc__': 'slimmedElectrons after basic selection (pt > 5 )'}, 'form_key': 'nElectron%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'delta eta (SC,ele) with sign'}, 'form_key': 'Electron_deltaEtaSC%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_deltaEtaSC%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'Non-PF Ecal isolation within a delta R cone of 0.3 with electron pt > 35 GeV'}, 'form_key': 'Electron_dr03EcalRecHitSumEt%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dr03EcalRecHitSumEt%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'Non-PF Hcal isolation within a delta R cone of 0.3 with electron pt > 35 GeV'}, 'form_key': 'Electron_dr03HcalDepth1TowerSumEt%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dr03HcalDepth1TowerSumEt%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'Non-PF track isolation within a delta R cone of 0.3 with electron pt > 35 GeV'}, 'form_key': 'Electron_dr03TkSumPt%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dr03TkSumPt%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'Non-PF track isolation within a delta R cone of 0.3 with electron pt > 35 GeV used in HEEP ID'}, 'form_key': 'Electron_dr03TkSumPtHEEP%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dr03TkSumPtHEEP%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'dxy (with sign) wrt first PV, in cm'}, 'form_key': 'Electron_dxy%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dxy%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'dxy uncertainty, in cm'}, 'form_key': 'Electron_dxyErr%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dxyErr%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'dz (with sign) wrt first PV, in cm'}, 'form_key': 'Electron_dz%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dz%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'dz uncertainty, in cm'}, 'form_key': 'Electron_dzErr%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_dzErr%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': '1/E_SC - 1/p_trk'}, 'form_key': 'Electron_eInvMinusPInv%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_eInvMinusPInv%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'energy error of the cluster-track combination'}, 'form_key': 'Electron_energyErr%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_energyErr%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'eta'}, 'form_key': 'Electron_eta%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_eta%2C%21load'}, {'class': 'ListOffsetArray', 'offsets': 'i64', 'content': {'class': 'NumpyArray', 'primitive': 'float32', 'inner_shape': [], 'parameters': {'__doc__': 'H over E'}, 'form_key': 'Electron_hoe%2C%21load%2C%21content'}, 'parameters': {}, 'form_key': 'Electron_hoe%2C%21load'}]\n",
      "File was loaded, fields count:  62\n",
      "Type of tree = uproot.open(file, **uproot_options):  <class 'uproot.models.RNTuple.Model_ROOT_3a3a_Experimental_3a3a_RNTuple'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model_ROOT_3a3a_Experimental_3a3a_RNTuple' object has no attribute 'num_entries'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile was loaded, fields count: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(events\u001b[38;5;241m.\u001b[39mfields))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# load_files_with_uproot(all_files)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mload_files_with_coffea\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mload_files_with_coffea\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_files_with_coffea\u001b[39m(files):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fl \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m---> 26\u001b[0m         events \u001b[38;5;241m=\u001b[39m \u001b[43mNanoEventsFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mfl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschemaclass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNanoAODSchema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mevents()\n\u001b[1;32m     27\u001b[0m         events_list\u001b[38;5;241m.\u001b[39mappend(events)\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile was loaded, fields count: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(events\u001b[38;5;241m.\u001b[39mfields))\n",
      "File \u001b[0;32m~/coffea/src/coffea/nanoevents/factory.py:368\u001b[0m, in \u001b[0;36mNanoEventsFactory.from_root\u001b[0;34m(cls, file, treepath, entry_start, entry_stop, steps_per_file, runtime_cache, persistent_cache, schemaclass, metadata, uproot_options, access_log, iteritems_options, use_ak_forth, delayed, known_base_form, decompression_executor, interpretation_executor)\u001b[0m\n\u001b[1;32m    366\u001b[0m     entry_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entry_stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m entry_stop \u001b[38;5;241m>\u001b[39m tree\u001b[38;5;241m.\u001b[39mnum_entries:\n\u001b[0;32m--> 368\u001b[0m     entry_stop \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_entries\u001b[49m\n\u001b[1;32m    371\u001b[0m partition_key \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mstr\u001b[39m(tree\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39muuid),\n\u001b[1;32m    373\u001b[0m     tree\u001b[38;5;241m.\u001b[39mobject_path,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry_stop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    376\u001b[0m uuidpfn \u001b[38;5;241m=\u001b[39m {partition_key[\u001b[38;5;241m0\u001b[39m]: tree\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39mfile_path}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model_ROOT_3a3a_Experimental_3a3a_RNTuple' object has no attribute 'num_entries'"
     ]
    }
   ],
   "source": [
    "all_files = []\n",
    "events_list = []\n",
    "\n",
    "# Some files are downloaded locally:\n",
    "# all_files.append(ttbar_file)\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\") # 533M size\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\") # RNTuple remote\n",
    "# all_files.append(\"root://eospublic.cern.ch//eos/root-eos/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\")\n",
    "all_files.append(\"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0007.root\") # TTree ttbar original\n",
    "# all_files.append(\"/home/cms-jovyan/my_root_files/ttree/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\") # TTree local\n",
    "all_files.append(\"/home/cms-jovyan/my_root_files/rntuple/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root\")  # RNTuple local\n",
    "\n",
    "\n",
    "\n",
    "def load_files_with_uproot(files):\n",
    "    for fl in files:\n",
    "        with uproot.open(fl) as f:\n",
    "            events = f[\"Events\"]\n",
    "            events_list.append(events)\n",
    "            print(\"File was loaded, event count: \", len(events.keys()))\n",
    "            \n",
    "            # NOTE: to access array: # events.arrays([\"Electron_pt\"])[\"Electron_pt\"]\n",
    "        \n",
    "def load_files_with_coffea(files):\n",
    "    for fl in files:\n",
    "        events = NanoEventsFactory.from_root({fl: \"Events\"}, schemaclass=NanoAODSchema, delayed=False).events()\n",
    "        events_list.append(events)\n",
    "        print(\"File was loaded, fields count: \", len(events.fields))\n",
    "        \n",
    "        \n",
    "# load_files_with_uproot(all_files)\n",
    "\n",
    "load_files_with_coffea(all_files)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58212611-abcb-42ed-96fc-9ad0a0e61d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting cell...\")\n",
    "\n",
    "file = all_files[1]\n",
    "\n",
    "events = events_list[1]\n",
    "\n",
    "\n",
    "# # Various available properties:\n",
    "# print(\"Name: \", events.name)\n",
    "# print(\"num_entries: \", events.num_entries)\n",
    "# print(\"header: \", events.header)\n",
    "# print(\"footer: \", events.footer)\n",
    "# print(\"field_names: \", events.field_names[:10])\n",
    "# print(\"column_records: \", events.column_records[:10])\n",
    "# print(\"keys: \", events.keys()[:10])\n",
    "# print(\"_column_records_dict: \", events._column_records_dict)\n",
    "# print(\"_related_ids: \", events._related_ids)\n",
    "# print(\"page_list_envelopes: \", events.page_list_envelopes)\n",
    "# print(\"keys: \", events.keys())\n",
    "\n",
    "\n",
    "# # Experimenting with array access:\n",
    "# array = events.arrays(filter_names=[\"Electron_pt\", \"Electron_eta\"])[:20]\n",
    "# print(\"events.arrays(): \", events.arrays())\n",
    "# print(\"----------\")\n",
    "# print(\"events.arrays().show(): \", events.arrays().show())\n",
    "# print(\"----------\")\n",
    "# # tree = uproot.open(file)\n",
    "\n",
    "# for key in  events.keys(): \n",
    "#     # branch = events.arrays([key])[key]\n",
    "#     print(key)\n",
    "\n",
    "\n",
    "## Things that do not work:\n",
    "# print(\"Show(): \", events.show()) # 'Model_ROOT_3a3a_Experimental_3a3a_RNTuple' object has no attribute 'show'\n",
    "# for key, branch in events.iteritems(): # Does not work with RNTuple. AttributeError: no field named 'iteritems'. However, it does work with TTree.\n",
    "# for key, branch in events.arrays().iteritems(): # Does not work both with TTree or RNTuple.\n",
    "# print(list(events.arrays().keys())) # Does not work. No field names keys()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0b5d9-cd7a-4f86-bca3-a2e8a15f9570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.088275Z",
     "start_time": "2024-08-01T15:10:40.085122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test times for each function:\n",
    "\n",
    "# import timeit\n",
    "\n",
    "# # Define the operations as functions\n",
    "# def array_1():\n",
    "#     print(events_list[0].arrays([\"Electron_pt\"])[\"Electron_pt\"])\n",
    "\n",
    "# def array_1_direct():\n",
    "#     print(events_list[0][\"Electron_pt\"].array())\n",
    "\n",
    "# def array_2():\n",
    "#     print(events_list[1].arrays([\"Electron_pt\"])[\"Electron_pt\"])\n",
    "\n",
    "# def array_2_direct():\n",
    "#     print(events_list[1][\"Electron_pt\"].array())\n",
    "\n",
    "# def array_3():\n",
    "#     print(events_list[2].arrays([\"Electron_pt\"])[\"Electron_pt\"])\n",
    "\n",
    "# def array_3_direct():\n",
    "#     print(events_list[2][\"Electron_pt\"].array())\n",
    "\n",
    "# # Time the operations\n",
    "# time_1 = timeit.timeit(array_1, number=1)\n",
    "# print(\"Time for events1.arrays(['Electron_pt'])['Electron_pt']: \", time_1)\n",
    "# time_1_direct = timeit.timeit(array_1_direct, number=1)\n",
    "# print(\"Time for events1['Electron_pt'].array(): \", time_1_direct)\n",
    "\n",
    "# print(\"*****\")\n",
    "\n",
    "# time_2 = timeit.timeit(array_2, number=1)\n",
    "# print(\"Time for events2.arrays(['Electron_pt'])['Electron_pt']: \", time_2)\n",
    "# time_2_direct = timeit.timeit(array_2_direct, number=1)\n",
    "# print(\"Time for events2['Electron_pt'].array(): \", time_2_direct)\n",
    "\n",
    "# print(\"*****\")\n",
    "\n",
    "# time_3 = timeit.timeit(array_3, number=1)\n",
    "# print(\"Time for events3.arrays(['Electron_pt'])['Electron_pt']: \", time_3)\n",
    "# time_3_direct = timeit.timeit(array_3_direct, number=1)\n",
    "# print(\"Time for events3['Electron_pt'].array(): \", time_3_direct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15f7ba-1f90-41b3-8f44-8e6f59b9974c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:10:40.203342Z",
     "start_time": "2024-08-01T15:10:40.106928Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the task graph to build a histogram\n",
    "print(\"Calculating trijet mass...\")\n",
    "reconstructed_top_mass = calculate_trijet_mass(events_list[0])\n",
    "print(\"hist_reco_mtop...\")\n",
    "hist_reco_mtop = hist.dask.Hist.new.Reg(16, 0, 375, label=\"$m_{bjj}$\").Double().fill(reconstructed_top_mass)\n",
    "print(\"Finished cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad62a29",
   "metadata": {},
   "source": [
    "and then once we're ready we can execute the task graph with `.compute()` to get our visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3d2d1-7b45-47f6-830d-7c46b479f7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T15:21:58.923162Z",
     "start_time": "2024-08-01T15:10:40.204505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform computation and visualize\n",
    "artists = hist_reco_mtop.compute().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998956ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and annotate the visualization\n",
    "fig_dir = Path.cwd() / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "ax.vlines(175, 0, 10000, colors=[\"grey\"], linestyle=\"dotted\")\n",
    "ax.text(180, 150, \"$m_{t} = 175$ GeV\")\n",
    "ax.set_xlim([0, 375])\n",
    "ax.set_ylim([0, 8000])\n",
    "\n",
    "fig.savefig(fig_dir / \"trijet_mass.png\", dpi=300)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2d6bd-b316-4627-a90c-a8c7e4a028e9",
   "metadata": {},
   "source": [
    "This all matches the (non-Dask) versions of the plots from last summer — see the notebook linked above. Not surprising, but reassuring!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb5a13-a8c0-4236-9c71-7ec6847773cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time for coffea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6684c8",
   "metadata": {},
   "source": [
    "We'll first write the functions to compute the observable and do the histogramming using `awkward-dask` and `hist.dask` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "# perform object selection\n",
    "def object_selection(events):\n",
    "    elecs = events.Electron\n",
    "    muons = events.Muon\n",
    "    jets = events.Jet\n",
    "\n",
    "    electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "    muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                 (muons.pfRelIso04_all < 0.15))\n",
    "    jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "    # Only keep objects that pass our requirements\n",
    "    elecs = elecs[electron_reqs]\n",
    "    muons = muons[muon_reqs]\n",
    "    jets = jets[jet_reqs]\n",
    "\n",
    "    return elecs, muons, jets\n",
    "\n",
    "\n",
    "# event selection for 4j1b and 4j2b\n",
    "def region_selection(elecs, muons, jets):\n",
    "    ######### Store boolean masks with PackedSelection ##########\n",
    "    selections = PackedSelection(dtype='uint64')\n",
    "    # Basic selection criteria\n",
    "    selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "    selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "    selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "    selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    # Complex selection criteria\n",
    "    selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "    selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "    return selections.all(\"4j1b\"), selections.all(\"4j2b\")\n",
    "\n",
    "\n",
    "# observable calculation for 4j2b\n",
    "def calculate_m_reco_top(jets):\n",
    "    # reconstruct hadronic top as bjj system with largest pT\n",
    "    trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2,\n",
    "                                    np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in candidates\n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "    return observable\n",
    "\n",
    "\n",
    "# create histograms with observables\n",
    "def create_histograms(events):\n",
    "    hist_4j1b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"HT\", label=r\"$H_T$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    hist_4j2b = (\n",
    "        hist.dask.Hist.new.Reg(25, 50, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "        .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "        .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "    variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "    process_label = events.metadata[\"process_label\"]  # nicer LaTeX labels\n",
    "\n",
    "    # normalization for MC\n",
    "    x_sec = events.metadata[\"xsec\"]\n",
    "    nevts_total = events.metadata[\"nevts\"]\n",
    "    lumi = 3378 # /pb\n",
    "    if process != \"data\":\n",
    "        xsec_weight = x_sec * lumi / nevts_total\n",
    "    else:\n",
    "        xsec_weight = 1\n",
    "\n",
    "    elecs, muons, jets = object_selection(events)\n",
    "\n",
    "    # region selection\n",
    "    selection_4j1b, selection_4j2b = region_selection(elecs, muons, jets)\n",
    "\n",
    "    # 4j1b: HT\n",
    "    observable_4j1b = ak.sum(jets[selection_4j1b].pt, axis=-1)\n",
    "    hist_4j1b.fill(observable_4j1b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    # 4j2b: m_reco_top\n",
    "    observable_4j2b = calculate_m_reco_top(jets[selection_4j2b])\n",
    "    hist_4j2b.fill(observable_4j2b, weight=xsec_weight, process=process_label, variation=variation)\n",
    "\n",
    "    return {\"4j1b\": hist_4j1b, \"4j2b\": hist_4j2b}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3816c2",
   "metadata": {},
   "source": [
    "and prepare the fileset we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b151e-d7a8-49d4-8368-310cbe149b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileset preparation\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "# compared to coffea 0.7: list of file paths becomes list of dicts (path: trename)\n",
    "fileset = utils.file_input.construct_fileset(N_FILES_MAX_PER_SAMPLE)\n",
    "\n",
    "# fileset = {\"ttbar__nominal\": fileset[\"ttbar__nominal\"]}  # to only process nominal ttbar\n",
    "# fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed65cdb",
   "metadata": {},
   "source": [
    "Now we can start using `coffea` with its Dask capabilities. One of the things we need to do is to build the full task graph, which requires looping over all the sample variations (`samples`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc41-0a72-44ec-a8b8-654286a02196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples, _ = dataset_tools.preprocess(fileset, step_size=250_000)\n",
    "\n",
    "# workaround for https://github.com/CoffeaTeam/coffea/issues/1050 (metadata gets dropped, already fixed)\n",
    "for k, v in samples.items():\n",
    "    v[\"metadata\"] = fileset[k][\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the task graph\n",
    "tasks = dataset_tools.apply_to_fileset(create_histograms, samples, uproot_options={\"allow_read_errors_with_report\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "and then we can finally execute the full task graph with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "((out, report),) = dask.compute(tasks)  # feels strange that this is a tuple-of-tuple\n",
    "\n",
    "print(f\"total time spent in uproot reading data (or some related metric?): {ak.sum([v['duration'] for v in report.values()]):.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c7fad",
   "metadata": {},
   "source": [
    "To visualize the results, we need to first stack the serperate histograms that were computed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714929f5-9c56-4afa-87e6-5d096af21ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stack all the histograms together (we processed each sample separately)\n",
    "full_histogram_4j1b = sum([v[\"4j1b\"] for v in out.values()])\n",
    "full_histogram_4j2b = sum([v[\"4j2b\"] for v in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c0147-6b4e-4ae7-b4e1-b8eb6b764c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j1b[120j::hist.rebin(2), :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, 1 b-tag\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_1b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba9e07-ec3c-4cdb-be16-4cab17e02a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = full_histogram_4j2b[:, :, \"nominal\"].stack(\"process\")[::-1].plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\"\n",
    ")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\">= 4 jets, >= 2 b-tags\");\n",
    "\n",
    "fig.savefig(fig_dir / \"coffea_4j_2b.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa67e0-be77-4f70-8a2d-02446ef7793c",
   "metadata": {},
   "source": [
    "This is a plot you can compare to the one in the full AGC notebook — you'll notice they look the same. Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a83e2",
   "metadata": {},
   "source": [
    "If we now investigate the task graph for the nominal $t\\bar{t}$ sample in the optimzied view, which hides from us some of the complexity of the graph we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9195b47-34d0-4947-9691-1563580c3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[0][\"ttbar__nominal\"][\"4j2b\"].visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fca3c-6e3e-42f8-a917-c843822b1f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"100 layers is a large task graph\" on IRIS-HEP Slack, 100 layers happen quickly!\n",
    "for region in [\"4j1b\", \"4j2b\"]:\n",
    "    for process, task in tasks[0].items():\n",
    "        print(f\"{process:>30} {region} {len(task[region].dask.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04c78a-926f-47fb-956c-ef2355213514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns getting read for a given task\n",
    "dak.necessary_columns(tasks[0][\"ttbar__nominal\"][\"4j2b\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
